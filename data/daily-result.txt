2021-12-07
ED2: An Environment Dynamics Decomposition Framework for World Model
  Construction
Model-based reinforcement learning methods achieve significant sample
efficiency in many tasks, but their performance is often limited by the
existence of the model error. To reduce the model error, previous works use a
single well-designed network to fit the entire environment dynamics, which
treats the environment dynamics as a black box. However, these methods lack to
consider the environmental decomposed property that the dynamics may contain
multiple sub-dynamics, which can be modeled separately, allowing us to
construct the world model more accurately. In this paper, we propose the
Environment Dynamics Decomposition (ED2), a novel world model construction
framework that models the environment in a decomposing manner. ED2 contains two
key components: sub-dynamics discovery (SD2) and dynamics decomposition
prediction (D2P). SD2 discovers the sub-dynamics in an environment and then D2P
constructs the decomposed world model following the sub-dynamics. ED2 can be
easily combined with existing MBRL algorithms and empirical results show that
ED2 significantly reduces the model error and boosts the performance of the
state-of-the-art MBRL algorithms on various tasks.

2021-12-10
Scalable Power Control/Beamforming in Heterogeneous Wireless Networks
  with Graph Neural Networks
Machine learning (ML) has been widely used for efficient resource allocation
(RA) in wireless networks. Although superb performance is achieved on small and
simple networks, most existing ML-based approaches are confronted with
difficulties when heterogeneity occurs and network size expands. In this paper,
specifically focusing on power control/beamforming (PC/BF) in heterogeneous
device-to-device (D2D) networks, we propose a novel unsupervised learning-based
framework named heterogeneous interference graph neural network (HIGNN) to
handle these challenges. First, we characterize diversified link features and
interference relations with heterogeneous graphs. Then, HIGNN is proposed to
empower each link to obtain its individual transmission scheme after limited
information exchange with neighboring links. It is noteworthy that HIGNN is
scalable to wireless networks of growing sizes with robust performance after
trained on small-sized networks. Numerical results show that compared with
state-of-the-art benchmarks, HIGNN achieves much higher execution efficiency
while providing strong performance.

2021-12-16
You Only Need End-to-End Training for Long-Tailed Recognition
The generalization gap on the long-tailed data sets is largely owing to most
categories only occupying a few training samples. Decoupled training achieves
better performance by training backbone and classifier separately. What causes
the poorer performance of end-to-end model training (e.g., logits margin-based
methods)? In this work, we identify a key factor that affects the learning of
the classifier: the channel-correlated features with low entropy before
inputting into the classifier. From the perspective of information theory, we
analyze why cross-entropy loss tends to produce highly correlated features on
the imbalanced data. In addition, we theoretically analyze and prove its
impacts on the gradients of classifier weights, the condition number of
Hessian, and logits margin-based approach. Therefore, we firstly propose to use
Channel Whitening to decorrelate ("scatter") the classifier's inputs for
decoupling the weight update and reshaping the skewed decision boundary, which
achieves satisfactory results combined with logits margin-based method.
However, when the number of minor classes are large, batch imbalance and more
participation in training cause over-fitting of the major classes. We also
propose two novel modules, Block-based Relatively Balanced Batch Sampler (B3RS)
and Batch Embedded Training (BET) to solve the above problems, which makes the
end-to-end training achieve even better performance than decoupled training.
Experimental results on the long-tailed classification benchmarks, CIFAR-LT and
ImageNet-LT, demonstrate the effectiveness of our method.

